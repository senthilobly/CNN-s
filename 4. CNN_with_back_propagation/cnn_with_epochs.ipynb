{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# CNN archietecture with MNIST like image set generated using numpy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "input = np.random.rand(1000, 28,28)\n",
        "y_true = np.random.randint(0,10, size=(1000,))\n",
        "\n",
        "# Hyper Parameter\n",
        "kernel = np.ones((3,3))*0.01\n",
        "stride=1\n",
        "max_pool = 2\n",
        "max_stride = 2\n",
        "padding=0\n",
        "weight = np.random.randn(169, 10)*0.01 # Output after Max pool will be 13x13 so we have 169x10 weights\n",
        "bias = np.zeros((10))\n",
        "learning_rate = 0.02\n",
        "epochs = 100\n",
        "num_samples = len(input)\n",
        "\n",
        "\n",
        "def conv(img, kernel, stride):\n",
        "  ker_h, ker_w = kernel.shape\n",
        "  img_h, img_w = img.shape\n",
        "  out_h = ((img_h - ker_h)//stride)+1\n",
        "  out_w = ((img_w - ker_w)//stride)+1\n",
        "  output = np.zeros((out_h, out_w))\n",
        "  for i in range(out_h):\n",
        "    for j in range(out_w):\n",
        "      region = img[i:i+ker_h, j:j+ker_w]\n",
        "      output[i,j] = np.sum(region * kernel)\n",
        "  return output\n",
        "\n",
        "def max_pooling(img, kernel, stride, pool):\n",
        "  img_h, img_w = img.shape\n",
        "  out_h = ((img_h - pool)//stride) + 1\n",
        "  out_w = ((img_w - pool)//stride) + 1\n",
        "  output = np.zeros((out_h, out_w))\n",
        "  for i in range(out_h):\n",
        "    for j in range(out_w):\n",
        "      region = img[i*stride:i*stride+pool, j*stride:j*stride+pool]\n",
        "      output[i,j] =  np.max(region)\n",
        "  return output\n",
        "\n",
        "def relu(x):\n",
        "   return np.maximum(0, x)\n",
        "\n",
        "def relu_der(x):\n",
        "  return (x > 0).astype(float)\n",
        "\n",
        "def one_hot(y, num_classes=10):\n",
        "    return np.eye(num_classes)[y]\n",
        "\n",
        "def flatten(x):\n",
        "    return x.reshape(-1)\n",
        "\n",
        "def softmax(x):\n",
        "  exps = np.exp(x - np.max(x))\n",
        "  return exps / np.sum(exps)\n",
        "\n",
        "def cross_entrophy(pred, target):\n",
        "  return -np.sum(target * np.log(pred + 1e-9))\n",
        "\n",
        "y_encoded = one_hot(y_true)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  total_loss = 0\n",
        "  correct = 0\n",
        "  print(\"Input length\", len(input))\n",
        "  for i in range(len(input)):\n",
        "    x = input[i]\n",
        "    y = y_encoded[i]\n",
        "    conv1 = conv(x, kernel, stride)\n",
        "    conv_activated = relu(conv1)\n",
        "    max_output = max_pooling(conv_activated, kernel, max_stride, max_pool)\n",
        "    flat_output = flatten(max_output)\n",
        "    predicted = np.dot(flat_output, weight) + bias\n",
        "    probability = softmax(predicted)\n",
        "\n",
        "    # Loss Calculation\n",
        "    loss = cross_entrophy(probability, y)\n",
        "    total_loss += loss\n",
        "    if np.argmax(probability) == np.argmax(y):\n",
        "      correct += 1\n",
        "\n",
        "    # Back Propagation\n",
        "    dL_dO = probability - y\n",
        "    dW = np.outer(flat_output, dL_dO)\n",
        "\n",
        "    # Update weights and Bias\n",
        "    weight -= learning_rate * dW\n",
        "    bias -= learning_rate * dL_dO\n",
        "  accuracy = correct / len(input)\n",
        "  print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f} | Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoCtDGTqZE_4",
        "outputId": "b8820810-e2e7-4c2c-beb7-7070d22cc418"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input length 1000\n",
            "Epoch 1 | Loss: 2306.2926 | Accuracy: 0.0910\n",
            "Input length 1000\n",
            "Epoch 2 | Loss: 2306.2020 | Accuracy: 0.1050\n",
            "Input length 1000\n",
            "Epoch 3 | Loss: 2306.0951 | Accuracy: 0.1050\n",
            "Input length 1000\n",
            "Epoch 4 | Loss: 2305.9234 | Accuracy: 0.1050\n",
            "Input length 1000\n",
            "Epoch 5 | Loss: 2305.7487 | Accuracy: 0.1050\n",
            "Input length 1000\n",
            "Epoch 6 | Loss: 2305.5738 | Accuracy: 0.1050\n",
            "Input length 1000\n",
            "Epoch 7 | Loss: 2305.3991 | Accuracy: 0.1050\n",
            "Input length 1000\n",
            "Epoch 8 | Loss: 2305.2244 | Accuracy: 0.1050\n",
            "Input length 1000\n",
            "Epoch 9 | Loss: 2305.0498 | Accuracy: 0.1050\n",
            "Input length 1000\n",
            "Epoch 10 | Loss: 2304.8752 | Accuracy: 0.1050\n",
            "Input length 1000\n",
            "Epoch 11 | Loss: 2304.7007 | Accuracy: 0.1050\n",
            "Input length 1000\n",
            "Epoch 12 | Loss: 2304.5263 | Accuracy: 0.1050\n",
            "Input length 1000\n",
            "Epoch 13 | Loss: 2304.3520 | Accuracy: 0.1050\n",
            "Input length 1000\n",
            "Epoch 14 | Loss: 2304.1777 | Accuracy: 0.1060\n",
            "Input length 1000\n",
            "Epoch 15 | Loss: 2304.0034 | Accuracy: 0.1060\n",
            "Input length 1000\n",
            "Epoch 16 | Loss: 2303.8293 | Accuracy: 0.1050\n",
            "Input length 1000\n",
            "Epoch 17 | Loss: 2303.6552 | Accuracy: 0.1050\n",
            "Input length 1000\n",
            "Epoch 18 | Loss: 2303.4812 | Accuracy: 0.1050\n",
            "Input length 1000\n",
            "Epoch 19 | Loss: 2303.3072 | Accuracy: 0.1050\n",
            "Input length 1000\n",
            "Epoch 20 | Loss: 2303.1333 | Accuracy: 0.1050\n",
            "Input length 1000\n",
            "Epoch 21 | Loss: 2302.9595 | Accuracy: 0.1050\n",
            "Input length 1000\n",
            "Epoch 22 | Loss: 2302.7858 | Accuracy: 0.1050\n",
            "Input length 1000\n",
            "Epoch 23 | Loss: 2302.6121 | Accuracy: 0.1060\n",
            "Input length 1000\n",
            "Epoch 24 | Loss: 2302.4384 | Accuracy: 0.1060\n",
            "Input length 1000\n",
            "Epoch 25 | Loss: 2302.2649 | Accuracy: 0.1060\n",
            "Input length 1000\n",
            "Epoch 26 | Loss: 2302.0914 | Accuracy: 0.1090\n",
            "Input length 1000\n",
            "Epoch 27 | Loss: 2301.9180 | Accuracy: 0.1090\n",
            "Input length 1000\n",
            "Epoch 28 | Loss: 2301.7446 | Accuracy: 0.1090\n",
            "Input length 1000\n",
            "Epoch 29 | Loss: 2301.5713 | Accuracy: 0.1090\n",
            "Input length 1000\n",
            "Epoch 30 | Loss: 2301.3981 | Accuracy: 0.1080\n",
            "Input length 1000\n",
            "Epoch 31 | Loss: 2301.2249 | Accuracy: 0.1080\n",
            "Input length 1000\n",
            "Epoch 32 | Loss: 2301.0518 | Accuracy: 0.1080\n",
            "Input length 1000\n",
            "Epoch 33 | Loss: 2300.8788 | Accuracy: 0.1080\n",
            "Input length 1000\n",
            "Epoch 34 | Loss: 2300.7059 | Accuracy: 0.1080\n",
            "Input length 1000\n",
            "Epoch 35 | Loss: 2300.5330 | Accuracy: 0.1070\n",
            "Input length 1000\n",
            "Epoch 36 | Loss: 2300.3601 | Accuracy: 0.1070\n",
            "Input length 1000\n",
            "Epoch 37 | Loss: 2300.1874 | Accuracy: 0.1070\n",
            "Input length 1000\n",
            "Epoch 38 | Loss: 2300.0147 | Accuracy: 0.1090\n",
            "Input length 1000\n",
            "Epoch 39 | Loss: 2299.8420 | Accuracy: 0.1090\n",
            "Input length 1000\n",
            "Epoch 40 | Loss: 2299.6695 | Accuracy: 0.1100\n",
            "Input length 1000\n",
            "Epoch 41 | Loss: 2299.4970 | Accuracy: 0.1090\n",
            "Input length 1000\n",
            "Epoch 42 | Loss: 2299.3245 | Accuracy: 0.1110\n",
            "Input length 1000\n",
            "Epoch 43 | Loss: 2299.1522 | Accuracy: 0.1110\n",
            "Input length 1000\n",
            "Epoch 44 | Loss: 2298.9799 | Accuracy: 0.1110\n",
            "Input length 1000\n",
            "Epoch 45 | Loss: 2298.8076 | Accuracy: 0.1110\n",
            "Input length 1000\n",
            "Epoch 46 | Loss: 2298.6355 | Accuracy: 0.1110\n",
            "Input length 1000\n",
            "Epoch 47 | Loss: 2298.4634 | Accuracy: 0.1110\n",
            "Input length 1000\n",
            "Epoch 48 | Loss: 2298.2913 | Accuracy: 0.1110\n",
            "Input length 1000\n",
            "Epoch 49 | Loss: 2298.1193 | Accuracy: 0.1110\n",
            "Input length 1000\n",
            "Epoch 50 | Loss: 2297.9474 | Accuracy: 0.1110\n",
            "Input length 1000\n",
            "Epoch 51 | Loss: 2297.7756 | Accuracy: 0.1110\n",
            "Input length 1000\n",
            "Epoch 52 | Loss: 2297.6038 | Accuracy: 0.1110\n",
            "Input length 1000\n",
            "Epoch 53 | Loss: 2297.4321 | Accuracy: 0.1110\n",
            "Input length 1000\n",
            "Epoch 54 | Loss: 2297.2605 | Accuracy: 0.1110\n",
            "Input length 1000\n",
            "Epoch 55 | Loss: 2297.0889 | Accuracy: 0.1110\n",
            "Input length 1000\n",
            "Epoch 56 | Loss: 2296.9174 | Accuracy: 0.1110\n",
            "Input length 1000\n",
            "Epoch 57 | Loss: 2296.7459 | Accuracy: 0.1110\n",
            "Input length 1000\n",
            "Epoch 58 | Loss: 2296.5745 | Accuracy: 0.1120\n",
            "Input length 1000\n",
            "Epoch 59 | Loss: 2296.4032 | Accuracy: 0.1120\n",
            "Input length 1000\n",
            "Epoch 60 | Loss: 2296.2320 | Accuracy: 0.1130\n",
            "Input length 1000\n",
            "Epoch 61 | Loss: 2296.0608 | Accuracy: 0.1130\n",
            "Input length 1000\n",
            "Epoch 62 | Loss: 2295.8896 | Accuracy: 0.1150\n",
            "Input length 1000\n",
            "Epoch 63 | Loss: 2295.7186 | Accuracy: 0.1150\n",
            "Input length 1000\n",
            "Epoch 64 | Loss: 2295.5476 | Accuracy: 0.1150\n",
            "Input length 1000\n",
            "Epoch 65 | Loss: 2295.3767 | Accuracy: 0.1150\n",
            "Input length 1000\n",
            "Epoch 66 | Loss: 2295.2058 | Accuracy: 0.1150\n",
            "Input length 1000\n",
            "Epoch 67 | Loss: 2295.0350 | Accuracy: 0.1150\n",
            "Input length 1000\n",
            "Epoch 68 | Loss: 2294.8643 | Accuracy: 0.1150\n",
            "Input length 1000\n",
            "Epoch 69 | Loss: 2294.6936 | Accuracy: 0.1160\n",
            "Input length 1000\n",
            "Epoch 70 | Loss: 2294.5230 | Accuracy: 0.1170\n",
            "Input length 1000\n",
            "Epoch 71 | Loss: 2294.3524 | Accuracy: 0.1180\n",
            "Input length 1000\n",
            "Epoch 72 | Loss: 2294.1820 | Accuracy: 0.1180\n",
            "Input length 1000\n",
            "Epoch 73 | Loss: 2294.0116 | Accuracy: 0.1180\n",
            "Input length 1000\n",
            "Epoch 74 | Loss: 2293.8412 | Accuracy: 0.1190\n",
            "Input length 1000\n",
            "Epoch 75 | Loss: 2293.6709 | Accuracy: 0.1200\n",
            "Input length 1000\n",
            "Epoch 76 | Loss: 2293.5007 | Accuracy: 0.1200\n",
            "Input length 1000\n",
            "Epoch 77 | Loss: 2293.3306 | Accuracy: 0.1190\n",
            "Input length 1000\n",
            "Epoch 78 | Loss: 2293.1605 | Accuracy: 0.1190\n",
            "Input length 1000\n",
            "Epoch 79 | Loss: 2292.9905 | Accuracy: 0.1190\n",
            "Input length 1000\n",
            "Epoch 80 | Loss: 2292.8205 | Accuracy: 0.1200\n",
            "Input length 1000\n",
            "Epoch 81 | Loss: 2292.6506 | Accuracy: 0.1200\n",
            "Input length 1000\n",
            "Epoch 82 | Loss: 2292.4808 | Accuracy: 0.1210\n",
            "Input length 1000\n",
            "Epoch 83 | Loss: 2292.3110 | Accuracy: 0.1210\n",
            "Input length 1000\n",
            "Epoch 84 | Loss: 2292.1413 | Accuracy: 0.1210\n",
            "Input length 1000\n",
            "Epoch 85 | Loss: 2291.9717 | Accuracy: 0.1210\n",
            "Input length 1000\n",
            "Epoch 86 | Loss: 2291.8021 | Accuracy: 0.1210\n",
            "Input length 1000\n",
            "Epoch 87 | Loss: 2291.6326 | Accuracy: 0.1210\n",
            "Input length 1000\n",
            "Epoch 88 | Loss: 2291.4632 | Accuracy: 0.1210\n",
            "Input length 1000\n",
            "Epoch 89 | Loss: 2291.2938 | Accuracy: 0.1210\n",
            "Input length 1000\n",
            "Epoch 90 | Loss: 2291.1245 | Accuracy: 0.1220\n",
            "Input length 1000\n",
            "Epoch 91 | Loss: 2290.9552 | Accuracy: 0.1220\n",
            "Input length 1000\n",
            "Epoch 92 | Loss: 2290.7860 | Accuracy: 0.1220\n",
            "Input length 1000\n",
            "Epoch 93 | Loss: 2290.6169 | Accuracy: 0.1220\n",
            "Input length 1000\n",
            "Epoch 94 | Loss: 2290.4479 | Accuracy: 0.1220\n",
            "Input length 1000\n",
            "Epoch 95 | Loss: 2290.2789 | Accuracy: 0.1220\n",
            "Input length 1000\n",
            "Epoch 96 | Loss: 2290.1099 | Accuracy: 0.1220\n",
            "Input length 1000\n",
            "Epoch 97 | Loss: 2289.9411 | Accuracy: 0.1220\n",
            "Input length 1000\n",
            "Epoch 98 | Loss: 2289.7723 | Accuracy: 0.1230\n",
            "Input length 1000\n",
            "Epoch 99 | Loss: 2289.6035 | Accuracy: 0.1240\n",
            "Input length 1000\n",
            "Epoch 100 | Loss: 2289.4349 | Accuracy: 0.1240\n"
          ]
        }
      ]
    }
  ]
}